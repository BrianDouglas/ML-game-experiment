{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "connected-generator",
   "metadata": {},
   "source": [
    "# 1. Create a custom gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exact-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "balanced-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fancy-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, size, game_length):\n",
    "        self.size = size\n",
    "        self.GAME_LENGTH = game_length\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=-1, high=1, shape=(self.size, self.size), dtype=np.int32)\n",
    "        self.state, self.player = self.createBoard()\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.time_remaining -= 1\n",
    "        done = False\n",
    "        \n",
    "        #evaluate move, save value of the new space before move then update the state\n",
    "        invalid_move = False\n",
    "        if Action(action) == Action.UP:\n",
    "            new_pos = (self.player[0] -1, self.player[1])\n",
    "            if new_pos[0] >= 0:\n",
    "                new_space_val = self.movePlayer(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.DOWN:\n",
    "            new_pos = (self.player[0] +1, self.player[1])\n",
    "            if new_pos[0] < self.size:\n",
    "                new_space_val = self.movePlayer(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.LEFT:\n",
    "            new_pos = (self.player[0], self.player[1] -1)\n",
    "            if new_pos[1] >= 0:\n",
    "                new_space_val = self.movePlayer(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.RIGHT:\n",
    "            new_pos = (self.player[0], self.player[1] +1)\n",
    "            if new_pos[1] < self.size:\n",
    "                new_space_val = self.movePlayer(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        else:\n",
    "            print(\"Invalid input to step function\")\n",
    "            \n",
    "        #evaluate reward \n",
    "        reward = 0\n",
    "        if(invalid_move):\n",
    "            reward = -0.1\n",
    "            done = False\n",
    "        else:\n",
    "            if new_space_val == 0:\n",
    "                reward = -0.1\n",
    "                done = False\n",
    "            elif new_space_val == -1:\n",
    "                reward = 20\n",
    "                done = True\n",
    "        \n",
    "        #evaluate if out of time\n",
    "        if self.time_remaining == 0:\n",
    "            done = True\n",
    "            reward = -20\n",
    "            \n",
    "        #placeholder for required return value\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def movePlayer(self, new_pos):\n",
    "        new_space_val = self.state[new_pos]\n",
    "        self.state[self.player] = 0\n",
    "        self.state[new_pos] = 1\n",
    "        self.player = new_pos\n",
    "        return new_space_val\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        print(self.state)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state, self.player =  self.createBoard()\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        return self.state\n",
    "    \n",
    "    def createBoard(self):\n",
    "        board = np.zeros((self.size,self.size), dtype=np.int32)\n",
    "        player_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "        goal_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "        player_goal_distance = math.sqrt((player_pos[0] - goal_pos[0])**2 + (player_pos[1] - goal_pos[1])**2)\n",
    "        while player_goal_distance < self.size/2:\n",
    "            goal_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            player_goal_distance = math.sqrt((player_pos[0] - goal_pos[0])**2 + (player_pos[1] - goal_pos[1])**2)\n",
    "        board[player_pos] = 1\n",
    "        board[goal_pos] = -1\n",
    "        return board, player_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-black",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "moving-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-21.900000000000002\n",
      "Episode:2 Score:-21.900000000000002\n",
      "Episode:3 Score:-21.900000000000002\n",
      "Episode:4 Score:19.4\n",
      "Episode:5 Score:-21.900000000000002\n",
      "Episode:6 Score:-21.900000000000002\n",
      "Episode:7 Score:-21.900000000000002\n",
      "Episode:8 Score:-21.900000000000002\n",
      "Episode:9 Score:-21.900000000000002\n",
      "Episode:10 Score:19.0\n",
      "Episode:11 Score:-21.900000000000002\n",
      "Episode:12 Score:-21.900000000000002\n",
      "Episode:13 Score:-21.900000000000002\n",
      "Episode:14 Score:-21.900000000000002\n",
      "Episode:15 Score:19.5\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv(5, 20)\n",
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    print(f'Episode:{episode} Score:{score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "trained-folder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  1  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  1  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  1  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 0  0  0  0 -1]\n",
      " [ 0  0  0  0  0]]\n",
      "Episode:15 Score:-21.900000000000002\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    env.render()\n",
    "\n",
    "print(f'Episode:{episode} Score:{score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-rough",
   "metadata": {},
   "source": [
    "# 2. Create Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "southern-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "sunset-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "exterior-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states[0],states[1])))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "thirty-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "better-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 13,104\n",
      "Trainable params: 13,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-rabbit",
   "metadata": {},
   "source": [
    "# 3. Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elder-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "#from tf_agents.environments import tf_py_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "linear-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "infectious-california",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -0.5578\n",
      "624 episodes - episode_reward: -8.938 [-21.900, 19.800] - loss: 9.020 - mae: 5.249 - mean_q: 2.662\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: 0.2379\n",
      "789 episodes - episode_reward: 3.017 [-21.900, 19.800] - loss: 12.698 - mae: 6.969 - mean_q: 7.306\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: 0.7709\n",
      "887 episodes - episode_reward: 8.690 [-21.900, 19.800] - loss: 13.369 - mae: 8.319 - mean_q: 10.708\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.0834\n",
      "939 episodes - episode_reward: 11.538 [-21.900, 19.800] - loss: 14.473 - mae: 9.174 - mean_q: 13.594\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 1.2517\n",
      "971 episodes - episode_reward: 12.891 [-21.900, 19.800] - loss: 14.468 - mae: 9.976 - mean_q: 15.093\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: 1.3220\n",
      "1002 episodes - episode_reward: 13.194 [-21.900, 19.800] - loss: 13.581 - mae: 10.427 - mean_q: 15.327\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 68s 7ms/step - reward: 1.2255\n",
      "954 episodes - episode_reward: 12.846 [-21.900, 19.800] - loss: 12.779 - mae: 10.652 - mean_q: 15.344\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.3258\n",
      "982 episodes - episode_reward: 13.501 [-21.900, 19.800] - loss: 11.929 - mae: 10.853 - mean_q: 15.650\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.1333\n",
      "932 episodes - episode_reward: 12.161 [-21.900, 19.800] - loss: 11.358 - mae: 10.901 - mean_q: 15.497\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.3619\n",
      "990 episodes - episode_reward: 13.757 [-21.900, 19.800] - loss: 10.784 - mae: 11.093 - mean_q: 15.493\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9008\n",
      "876 episodes - episode_reward: 10.284 [-21.900, 19.800] - loss: 9.279 - mae: 11.325 - mean_q: 15.541\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9849\n",
      "890 episodes - episode_reward: 11.065 [-21.900, 19.800] - loss: 8.651 - mae: 11.426 - mean_q: 15.590\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 0.8706\n",
      "859 episodes - episode_reward: 10.136 [-21.900, 19.800] - loss: 8.434 - mae: 11.586 - mean_q: 15.540\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.0811\n",
      "912 episodes - episode_reward: 11.854 [-21.900, 19.800] - loss: 8.731 - mae: 11.650 - mean_q: 15.439\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9868\n",
      "881 episodes - episode_reward: 11.202 [-21.900, 19.800] - loss: 8.848 - mae: 11.849 - mean_q: 15.592\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1132\n",
      "922 episodes - episode_reward: 12.074 [-21.900, 19.800] - loss: 9.536 - mae: 11.806 - mean_q: 16.196\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.0851\n",
      "914 episodes - episode_reward: 11.872 [-21.900, 19.800] - loss: 10.908 - mae: 12.042 - mean_q: 17.300\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1670\n",
      "903 episodes - episode_reward: 12.924 [-21.900, 19.800] - loss: 11.471 - mae: 12.102 - mean_q: 17.305\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1412: 0s - rew\n",
      "918 episodes - episode_reward: 12.432 [-21.900, 19.800] - loss: 11.106 - mae: 12.277 - mean_q: 17.499\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.0448\n",
      "884 episodes - episode_reward: 11.818 [-21.900, 19.800] - loss: 11.652 - mae: 12.165 - mean_q: 17.277\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 1.1051: 0\n",
      "908 episodes - episode_reward: 12.171 [-21.900, 19.800] - loss: 11.317 - mae: 12.320 - mean_q: 17.438\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9887\n",
      "866 episodes - episode_reward: 11.416 [-21.900, 19.800] - loss: 11.007 - mae: 12.320 - mean_q: 17.408\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9608\n",
      "878 episodes - episode_reward: 10.942 [-21.900, 19.800] - loss: 10.997 - mae: 12.280 - mean_q: 17.334\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9428\n",
      "885 episodes - episode_reward: 10.654 [-21.900, 19.800] - loss: 10.970 - mae: 12.272 - mean_q: 17.306\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.0107\n",
      "875 episodes - episode_reward: 11.553 [-21.900, 19.800] - loss: 11.086 - mae: 12.336 - mean_q: 17.396\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.9848\n",
      "880 episodes - episode_reward: 11.189 [-21.900, 19.800] - loss: 11.030 - mae: 12.369 - mean_q: 17.379\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1951\n",
      "915 episodes - episode_reward: 13.062 [-21.900, 19.800] - loss: 11.400 - mae: 12.348 - mean_q: 17.363\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.0269\n",
      "891 episodes - episode_reward: 11.525 [-21.900, 19.800] - loss: 11.397 - mae: 12.347 - mean_q: 17.351\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1412\n",
      "920 episodes - episode_reward: 12.404 [-21.900, 19.800] - loss: 11.290 - mae: 12.296 - mean_q: 17.258\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 1.0150\n",
      "899 episodes - episode_reward: 11.291 [-21.900, 19.800] - loss: 11.332 - mae: 12.324 - mean_q: 17.300\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1711\n",
      "915 episodes - episode_reward: 12.799 [-21.900, 19.800] - loss: 11.288 - mae: 12.390 - mean_q: 17.391\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.0732\n",
      "916 episodes - episode_reward: 11.716 [-21.900, 19.800] - loss: 11.260 - mae: 12.486 - mean_q: 17.514\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1173\n",
      "926 episodes - episode_reward: 12.065 [-21.900, 19.800] - loss: 11.343 - mae: 12.435 - mean_q: 17.440\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1693\n",
      "926 episodes - episode_reward: 12.628 [-21.900, 19.800] - loss: 11.079 - mae: 12.470 - mean_q: 17.461\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1250\n",
      "904 episodes - episode_reward: 12.444 [-21.900, 19.800] - loss: 11.001 - mae: 12.482 - mean_q: 17.462\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 0.8665\n",
      "853 episodes - episode_reward: 10.158 [-21.900, 19.800] - loss: 10.939 - mae: 12.478 - mean_q: 17.423\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: 1.1851\n",
      "912 episodes - episode_reward: 12.995 [-21.900, 19.800] - loss: 11.010 - mae: 12.394 - mean_q: 17.298\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 74s 7ms/step - reward: 1.0249\n",
      "888 episodes - episode_reward: 11.541 [-21.900, 19.800] - loss: 11.063 - mae: 12.361 - mean_q: 17.270\n",
      "\n",
      "Interval 39 (380000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0027\n",
      "873 episodes - episode_reward: 11.486 [-21.900, 19.800] - loss: 10.757 - mae: 12.477 - mean_q: 17.404\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0649\n",
      "886 episodes - episode_reward: 12.019 [-21.900, 19.800] - loss: 10.713 - mae: 12.411 - mean_q: 17.287\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0849\n",
      "892 episodes - episode_reward: 12.163 [-21.900, 19.800] - loss: 10.656 - mae: 12.380 - mean_q: 17.277\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.1049\n",
      "888 episodes - episode_reward: 12.442 [-21.900, 19.800] - loss: 10.650 - mae: 12.386 - mean_q: 17.257\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0830\n",
      "905 episodes - episode_reward: 11.968 [-21.900, 19.800] - loss: 10.650 - mae: 12.384 - mean_q: 17.252\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 0.9566\n",
      "862 episodes - episode_reward: 11.097 [-21.900, 19.800] - loss: 10.701 - mae: 12.456 - mean_q: 17.378\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0247\n",
      "872 episodes - episode_reward: 11.751 [-21.900, 19.800] - loss: 10.731 - mae: 12.441 - mean_q: 17.350\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 0.9827\n",
      "873 episodes - episode_reward: 11.258 [-21.900, 19.800] - loss: 10.959 - mae: 12.372 - mean_q: 17.261\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 0.9948\n",
      "885 episodes - episode_reward: 11.240 [-21.900, 19.800] - loss: 10.848 - mae: 12.396 - mean_q: 17.297\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.0551\n",
      "911 episodes - episode_reward: 11.581 [-21.900, 19.800] - loss: 11.144 - mae: 12.322 - mean_q: 17.225\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.0971\n",
      "908 episodes - episode_reward: 12.082 [-21.900, 19.800] - loss: 10.858 - mae: 12.359 - mean_q: 17.266\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.2134\n",
      "940 episodes - episode_reward: 12.910 [-21.900, 19.800] - loss: 10.904 - mae: 12.290 - mean_q: 17.171\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0911\n",
      "911 episodes - episode_reward: 11.976 [-21.900, 19.800] - loss: 11.079 - mae: 12.393 - mean_q: 17.320\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 72s 7ms/step - reward: 1.2275\n",
      "953 episodes - episode_reward: 12.880 [-21.900, 19.800] - loss: 11.028 - mae: 12.356 - mean_q: 17.269\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: 1.0369\n",
      "888 episodes - episode_reward: 11.678 [-21.900, 19.800] - loss: 10.819 - mae: 12.345 - mean_q: 17.241\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.1191\n",
      "909 episodes - episode_reward: 12.310 [-21.900, 19.800] - loss: 10.807 - mae: 12.432 - mean_q: 17.365\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1369\n",
      "892 episodes - episode_reward: 12.745 [-21.900, 19.800] - loss: 10.848 - mae: 12.313 - mean_q: 17.188\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.0569\n",
      "890 episodes - episode_reward: 11.877 [-21.900, 19.800] - loss: 10.955 - mae: 12.333 - mean_q: 17.235\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 0.9387\n",
      "873 episodes - episode_reward: 10.752 [-21.900, 19.800] - loss: 10.599 - mae: 12.406 - mean_q: 17.325\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1331\n",
      "908 episodes - episode_reward: 12.479 [-21.900, 19.800] - loss: 10.671 - mae: 12.363 - mean_q: 17.271\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.0608\n",
      "880 episodes - episode_reward: 12.056 [-21.900, 19.800] - loss: 10.748 - mae: 12.398 - mean_q: 17.343\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1630\n",
      "901 episodes - episode_reward: 12.906 [-21.900, 19.800] - loss: 10.652 - mae: 12.390 - mean_q: 17.329\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1190\n",
      "899 episodes - episode_reward: 12.447 [-21.900, 19.800] - loss: 10.531 - mae: 12.366 - mean_q: 17.247\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: 1.1871\n",
      "915 episodes - episode_reward: 12.975 [-21.900, 19.800] - loss: 10.343 - mae: 12.417 - mean_q: 17.320\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.1291\n",
      "914 episodes - episode_reward: 12.353 [-21.900, 19.800] - loss: 10.694 - mae: 12.447 - mean_q: 17.382\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.0409\n",
      "894 episodes - episode_reward: 11.644 [-21.900, 19.800] - loss: 10.514 - mae: 12.429 - mean_q: 17.340\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.2834\n",
      "943 episodes - episode_reward: 13.610 [-21.900, 19.800] - loss: 10.312 - mae: 12.488 - mean_q: 17.412\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.1934\n",
      "936 episodes - episode_reward: 12.750 [-21.900, 19.800] - loss: 10.167 - mae: 12.565 - mean_q: 17.503\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.1870\n",
      "905 episodes - episode_reward: 13.117 [-21.900, 19.800] - loss: 10.103 - mae: 12.509 - mean_q: 17.404\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.1472\n",
      "923 episodes - episode_reward: 12.429 [-21.900, 19.800] - loss: 10.121 - mae: 12.514 - mean_q: 17.406\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 82s 8ms/step - reward: 1.1993\n",
      "931 episodes - episode_reward: 12.882 [-21.900, 19.800] - loss: 9.887 - mae: 12.526 - mean_q: 17.428\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1090:\n",
      "904 episodes - episode_reward: 12.267 [-21.900, 19.800] - loss: 9.747 - mae: 12.626 - mean_q: 17.564\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1392\n",
      "917 episodes - episode_reward: 12.423 [-21.900, 19.800] - loss: 10.274 - mae: 12.502 - mean_q: 17.410\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1311\n",
      "907 episodes - episode_reward: 12.471 [-21.900, 19.800] - loss: 10.089 - mae: 12.503 - mean_q: 17.379\n",
      "\n",
      "Interval 73 (720000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1992\n",
      "919 episodes - episode_reward: 13.048 [-21.900, 19.800] - loss: 10.069 - mae: 12.571 - mean_q: 17.474\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1612\n",
      "918 episodes - episode_reward: 12.649 [-21.900, 19.800] - loss: 10.099 - mae: 12.573 - mean_q: 17.485\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1911\n",
      "911 episodes - episode_reward: 13.075 [-21.900, 19.800] - loss: 10.105 - mae: 12.585 - mean_q: 17.502\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 81s 8ms/step - reward: 1.1030\n",
      "905 episodes - episode_reward: 12.189 [-21.900, 19.800] - loss: 10.124 - mae: 12.525 - mean_q: 17.415\n",
      "\n",
      "Interval 77 (760000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.1410\n",
      "898 episodes - episode_reward: 12.706 [-21.900, 19.800] - loss: 10.276 - mae: 12.625 - mean_q: 17.562\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.1110\n",
      "901 episodes - episode_reward: 12.331 [-21.900, 19.800] - loss: 10.452 - mae: 12.487 - mean_q: 17.378\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.0869\n",
      "889 episodes - episode_reward: 12.227 [-21.900, 19.800] - loss: 10.115 - mae: 12.499 - mean_q: 17.371\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.0088\n",
      "880 episodes - episode_reward: 11.464 [-21.900, 19.800] - loss: 10.173 - mae: 12.504 - mean_q: 17.383\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.1091\n",
      "912 episodes - episode_reward: 12.161 [-21.900, 19.800] - loss: 10.417 - mae: 12.434 - mean_q: 17.304\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.0970\n",
      "900 episodes - episode_reward: 12.189 [-21.900, 19.800] - loss: 10.351 - mae: 12.481 - mean_q: 17.381\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.1553\n",
      "929 episodes - episode_reward: 12.436 [-21.900, 19.800] - loss: 10.257 - mae: 12.482 - mean_q: 17.374\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.0390\n",
      "903 episodes - episode_reward: 11.507 [-21.900, 19.800] - loss: 10.654 - mae: 12.387 - mean_q: 17.251\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.0852\n",
      "916 episodes - episode_reward: 11.846 [-21.900, 19.800] - loss: 10.752 - mae: 12.451 - mean_q: 17.347\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.3657: 0s - reward: 1.36\n",
      "970 episodes - episode_reward: 14.080 [-21.900, 19.800] - loss: 10.774 - mae: 12.441 - mean_q: 17.362\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.1050\n",
      "904 episodes - episode_reward: 12.224 [-21.900, 19.800] - loss: 10.678 - mae: 12.470 - mean_q: 17.394\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.2053\n",
      "926 episodes - episode_reward: 13.015 [-21.900, 19.800] - loss: 10.430 - mae: 12.484 - mean_q: 17.411\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.2774\n",
      "940 episodes - episode_reward: 13.589 [-21.900, 19.800] - loss: 10.462 - mae: 12.405 - mean_q: 17.307\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.2293\n",
      "928 episodes - episode_reward: 13.248 [-21.900, 19.800] - loss: 10.497 - mae: 12.496 - mean_q: 17.438\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: 1.1190\n",
      "901 episodes - episode_reward: 12.419 [-21.900, 19.800] - loss: 9.991 - mae: 12.597 - mean_q: 17.573\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: 1.0569\n",
      "890 episodes - episode_reward: 11.875 [-21.900, 19.800] - loss: 10.028 - mae: 12.604 - mean_q: 17.578\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: 0.8525\n",
      "846 episodes - episode_reward: 10.077 [-21.900, 19.800] - loss: 12.093 - mae: 13.424 - mean_q: 18.732\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -0.1155\n",
      "655 episodes - episode_reward: -1.762 [-21.900, 19.800] - loss: 81.490 - mae: 24.003 - mean_q: 33.484\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.6543\n",
      "572 episodes - episode_reward: -11.440 [-21.900, 19.800] - loss: 2454.538 - mae: 86.312 - mean_q: 122.378\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.9788\n",
      "522 episodes - episode_reward: -18.748 [-21.900, 19.700] - loss: 68978.930 - mae: 716.616 - mean_q: 1009.358\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.8547\n",
      "534 episodes - episode_reward: -16.006 [-21.900, 19.800] - loss: 513865.781 - mae: 2586.354 - mean_q: 3622.350\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.8167\n",
      "535 episodes - episode_reward: -15.266 [-21.900, 19.800] - loss: 2886045.250 - mae: 6914.280 - mean_q: 9656.741\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.7645\n",
      "547 episodes - episode_reward: -13.976 [-21.900, 19.800] - loss: 9832005.000 - mae: 14208.161 - mean_q: 19778.896\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -0.7506\n",
      "done, took 7532.374 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1105c93d080>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env =  tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "expanded-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: -21.900, steps: 20\n",
      "Episode 2: reward: -21.900, steps: 20\n",
      "Episode 3: reward: -21.900, steps: 20\n",
      "Episode 4: reward: -21.900, steps: 20\n",
      "Episode 5: reward: -21.900, steps: 20\n",
      "Episode 6: reward: -21.900, steps: 20\n",
      "Episode 7: reward: -21.900, steps: 20\n",
      "Episode 8: reward: -21.900, steps: 20\n",
      "Episode 9: reward: -21.900, steps: 20\n",
      "Episode 10: reward: -21.900, steps: 20\n",
      "Episode 11: reward: -21.900, steps: 20\n",
      "Episode 12: reward: -21.900, steps: 20\n",
      "Episode 13: reward: -21.900, steps: 20\n",
      "Episode 14: reward: -21.900, steps: 20\n",
      "Episode 15: reward: -21.900, steps: 20\n",
      "-21.899999999999995\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=15, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-rwanda",
   "metadata": {},
   "source": [
    "# 4. Saving and Reloading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "right-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('saved_models/78_perc_success.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-survival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:final_project] *",
   "language": "python",
   "name": "conda-env-final_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
