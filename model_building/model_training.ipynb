{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "connected-generator",
   "metadata": {},
   "source": [
    "# 1. Create a custom gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from enum import Enum\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "balanced-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fancy-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, size = 10, mode='static'):\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.GAME_LENGTH = 2 * size**2\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=0, high=3, shape=(self.size, self.size), dtype=np.int32)\n",
    "        self.state, self.player, self.goals = self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        self.episode = -1\n",
    "        #self.make_fig_dir()\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.time_remaining -= 1\n",
    "        done = False\n",
    "        \n",
    "        #evaluate move, save value of the new space before move then update the state\n",
    "        invalid_move = False\n",
    "        improved = False\n",
    "        if Action(action) == Action.UP:\n",
    "            new_pos = (self.player[0] -1, self.player[1])\n",
    "            if new_pos[0] >= 0 and self.state[new_pos] != 1:\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.DOWN:\n",
    "            new_pos = (self.player[0] +1, self.player[1])\n",
    "            if new_pos[0] < self.size and self.state[new_pos] != 1:\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.LEFT:\n",
    "            new_pos = (self.player[0], self.player[1] -1)\n",
    "            if new_pos[1] >= 0 and self.state[new_pos] != 1:\n",
    "                new_space_val= self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.RIGHT:\n",
    "            new_pos = (self.player[0], self.player[1] +1)\n",
    "            if new_pos[1] < self.size and self.state[new_pos] != 1:\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        else:\n",
    "            print(\"Invalid input to step function\")\n",
    "            \n",
    "        #evaluate reward \n",
    "        reward = 0\n",
    "        if invalid_move:\n",
    "            reward = -0.5\n",
    "            done = False\n",
    "        else:\n",
    "            if new_space_val == 0:\n",
    "                reward = -0.04\n",
    "                done = False\n",
    "            elif new_space_val == 3:\n",
    "                self.goals_remaining -= 1\n",
    "                if self.goals_remaining == 0:\n",
    "                    reward = 20\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 20\n",
    "                    done = False\n",
    "        \n",
    "        #evaluate if out of time\n",
    "        if self.time_remaining == 0:\n",
    "            done = True\n",
    "            reward = -20\n",
    "            \n",
    "        #placeholder for required return value\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def distanceToGoalImproved(self, new_pos):\n",
    "        distances_before = []\n",
    "        distances_after = []\n",
    "        for goal in self.goals:\n",
    "            distances_before.append(math.sqrt((self.player[0] - goal[0])**2 + (self.player[1] - goal[1])**2))\n",
    "            distances_after.append(math.sqrt((new_pos[0] - goal[0])**2 + (new_pos[1] - goal[1])**2))\n",
    "        index_of_closest = distances_before.index(min(distances_before))\n",
    "        distance_change = distances_after[index_of_closest] - distances_before[index_of_closest]\n",
    "        if distance_change < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def evalMove(self, new_pos):\n",
    "        new_space_val = self.state[new_pos]\n",
    "        self.state[self.player] = 0\n",
    "        self.state[new_pos] = 2\n",
    "        self.player = new_pos\n",
    "        return new_space_val\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Clock: {self.time_remaining}, Goals: {self.goals_remaining}')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(self.state)\n",
    "        #if self.episode % 5 == 0:\n",
    "        #    plt.savefig(f\"saved_figs//ep_{self.episode}/{self.GAME_LENGTH - self.time_remaining}_step\")\n",
    "        plt.show()\n",
    "    \n",
    "    def make_fig_dir(self):\n",
    "        cwd = os.getcwd()\n",
    "        path = os.path.join(cwd,f\"saved_figs/ep_{self.episode}\")\n",
    "        os.mkdir(path)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state, self.player, self.goals =  self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        self.episode += 1\n",
    "        #if self.episode % 5 == 0:\n",
    "        #    self.make_fig_dir()\n",
    "        return self.state\n",
    "    \n",
    "    def createBoard(self):\n",
    "        board = np.matrix([[1,1,1,1,1,1,1,1,1,1],\n",
    "                           [1,0,0,0,0,0,0,0,0,1],\n",
    "                           [1,1,1,0,1,1,1,0,1,1],\n",
    "                           [1,0,0,0,0,1,0,0,0,1],\n",
    "                           [1,0,1,1,1,1,0,1,0,1],\n",
    "                           [1,0,1,1,3,1,0,1,0,1],\n",
    "                           [1,0,0,0,0,0,0,1,0,1],\n",
    "                           [1,1,1,0,1,0,1,1,0,1],\n",
    "                           [1,3,0,0,1,0,0,0,3,1],\n",
    "                           [1,1,1,1,1,1,1,1,1,1]])\n",
    "        if self.mode == 'static':\n",
    "            player_pos = (1,1)\n",
    "            goals = [(5,5),(8,1),(8,8)]\n",
    "        elif self.mode == 'random':\n",
    "            player_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            goals = [(np.random.randint(self.size), np.random.randint(self.size))]\n",
    "            player_goal_distance = math.sqrt((player_pos[0] - goals[0][0])**2 + (player_pos[1] - goals[0][1])**2)\n",
    "            while player_goal_distance < self.size/2:\n",
    "                goals[0] = [(np.random.randint(self.size), np.random.randint(self.size))]\n",
    "                player_goal_distance = math.sqrt((player_pos[0] - goals[0][0])**2 + (player_pos[1] - goals[0][1])**2)    \n",
    "        \n",
    "        board[player_pos] = 2\n",
    "        for coord in goals:\n",
    "            if board[coord] == 0:\n",
    "                board[coord] = 3\n",
    "        return board, player_pos, goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-black",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "moving-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-75.33999999999992 Moves:200\n",
      "Episode:2 Score:-72.57999999999993 Moves:200\n",
      "Episode:3 Score:-76.25999999999993 Moves:200\n",
      "found one\n",
      "Episode:4 Score:-53.45999999999994 Moves:200\n",
      "found one\n",
      "Episode:5 Score:-54.83999999999994 Moves:200\n",
      "Episode:6 Score:-75.79999999999993 Moves:200\n",
      "found one\n",
      "Episode:7 Score:-48.859999999999914 Moves:200\n",
      "Episode:8 Score:-74.41999999999993 Moves:200\n",
      "found one\n",
      "Episode:9 Score:-58.519999999999925 Moves:200\n",
      "found one\n",
      "Episode:10 Score:-55.75999999999993 Moves:200\n",
      "found one\n",
      "Episode:11 Score:-51.159999999999926 Moves:200\n",
      "found one\n",
      "Episode:12 Score:-57.13999999999993 Moves:200\n",
      "found one\n",
      "Episode:13 Score:-52.99999999999992 Moves:200\n",
      "Episode:14 Score:-68.89999999999992 Moves:200\n",
      "Episode:15 Score:-84.07999999999996 Moves:200\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    num_moves = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        if reward == 20:\n",
    "            print(\"found one\")\n",
    "        score += reward\n",
    "        num_moves += 1\n",
    "        \n",
    "    print(f'Episode:{episode} Score:{score} Moves:{num_moves}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "trained-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:15 Score:-48.39999999999991\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    env.render()\n",
    "\n",
    "print(f'Episode:{episode} Score:{score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-rough",
   "metadata": {},
   "source": [
    "# 2. Create Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states[0],states[1])))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-rabbit",
   "metadata": {},
   "source": [
    "# 3. Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "#from tf_agents.environments import tf_py_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-california",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#env =  tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_tester(dqn, num_eps, vis = False):\n",
    "    scores = dqn.test(env, nb_episodes=num_eps, visualize=vis)\n",
    "    print(\"Mean Reward: \" + str(np.mean(scores.history['episode_reward'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tester(dqn, 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-rwanda",
   "metadata": {},
   "source": [
    "# 4. Saving and Reloading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('saved_models/10x10_maze_easy_redux.h5f', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv(size = 10)\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('saved_models/50-50_success.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-artwork",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn_tester(dqn, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tester(dqn, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-prompt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:final_project] *",
   "language": "python",
   "name": "conda-env-final_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
