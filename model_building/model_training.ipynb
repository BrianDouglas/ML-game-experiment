{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "connected-generator",
   "metadata": {},
   "source": [
    "# 1. Create a custom gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from enum import Enum\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, size = 10, mode='static'):\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.GAME_LENGTH = 2 * size**2\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=0, high=3, shape=(self.size, self.size), dtype=np.int32)\n",
    "        self.state, self.player, self.goals = self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        self.episode = -1\n",
    "        #self.make_fig_dir()\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.time_remaining -= 1\n",
    "        done = False\n",
    "        \n",
    "        #evaluate move, save value of the new space before move then update the state\n",
    "        invalid_move = False\n",
    "        improved = False\n",
    "        if Action(action) == Action.UP:\n",
    "            new_pos = (self.player[0] -1, self.player[1])\n",
    "            if new_pos[0] >= 0 and self.state[new_pos] != 1:\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.DOWN:\n",
    "            new_pos = (self.player[0] +1, self.player[1])\n",
    "            if new_pos[0] < self.size and self.state[new_pos] != 1:\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.LEFT:\n",
    "            new_pos = (self.player[0], self.player[1] -1)\n",
    "            if new_pos[1] >= 0 and self.state[new_pos] != 1:\n",
    "                new_space_val= self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.RIGHT:\n",
    "            new_pos = (self.player[0], self.player[1] +1)\n",
    "            if new_pos[1] < self.size and self.state[new_pos] != 1:\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        else:\n",
    "            print(\"Invalid input to step function\")\n",
    "            \n",
    "        #evaluate reward \n",
    "        reward = 0\n",
    "        if invalid_move:\n",
    "            reward = -0.5\n",
    "            done = False\n",
    "        else:\n",
    "            if new_space_val == 0:\n",
    "                reward = -0.04\n",
    "                done = False\n",
    "            elif new_space_val == 3:\n",
    "                self.goals_remaining -= 1\n",
    "                if self.goals_remaining == 0:\n",
    "                    reward = 20\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 20\n",
    "                    done = False\n",
    "        \n",
    "        #evaluate if out of time\n",
    "        if self.time_remaining == 0:\n",
    "            done = True\n",
    "            reward = -20\n",
    "            \n",
    "        #placeholder for required return value\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def distanceToGoalImproved(self, new_pos):\n",
    "        distances_before = []\n",
    "        distances_after = []\n",
    "        for goal in self.goals:\n",
    "            distances_before.append(math.sqrt((self.player[0] - goal[0])**2 + (self.player[1] - goal[1])**2))\n",
    "            distances_after.append(math.sqrt((new_pos[0] - goal[0])**2 + (new_pos[1] - goal[1])**2))\n",
    "        index_of_closest = distances_before.index(min(distances_before))\n",
    "        distance_change = distances_after[index_of_closest] - distances_before[index_of_closest]\n",
    "        if distance_change < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def evalMove(self, new_pos):\n",
    "        new_space_val = self.state[new_pos]\n",
    "        self.state[self.player] = 0\n",
    "        self.state[new_pos] = 2\n",
    "        self.player = new_pos\n",
    "        return new_space_val\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Clock: {self.time_remaining}, Goals: {self.goals_remaining}')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(self.state)\n",
    "        #if self.episode % 5 == 0:\n",
    "        #    plt.savefig(f\"saved_figs/ep_{self.episode}/{self.GAME_LENGTH - self.time_remaining}_step\")\n",
    "        plt.show()\n",
    "    \n",
    "    def make_fig_dir(self):\n",
    "        cwd = os.getcwd()\n",
    "        path = os.path.join(cwd,f\"saved_figs/ep_{self.episode}\")\n",
    "        os.mkdir(path)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state, self.player, self.goals =  self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        self.episode += 1\n",
    "        #if self.episode % 5 == 0:\n",
    "        #    self.make_fig_dir()\n",
    "        return self.state\n",
    "    \n",
    "    def createBoard(self):\n",
    "        board = np.matrix([[1,1,1,1,1,1,1,1,1,1],\n",
    "                           [1,0,0,0,0,0,0,0,0,1],\n",
    "                           [1,1,1,0,1,1,1,0,1,1],\n",
    "                           [1,0,0,0,0,1,0,0,0,1],\n",
    "                           [1,0,1,1,1,1,0,1,0,1],\n",
    "                           [1,0,1,1,3,1,0,1,0,1],\n",
    "                           [1,0,0,0,0,0,0,1,0,1],\n",
    "                           [1,1,1,0,1,0,1,1,0,1],\n",
    "                           [1,3,0,0,1,0,0,0,3,1],\n",
    "                           [1,1,1,1,1,1,1,1,1,1]])\n",
    "        if self.mode == 'static':\n",
    "            player_pos = (1,1)\n",
    "            goals = [(5,5),(8,1),(8,8)]\n",
    "        elif self.mode == 'random':\n",
    "            player_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            goal_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            player_goal_distance = math.sqrt((player_pos[0] - goal_pos[0])**2 + (player_pos[1] - goal_pos[1])**2)\n",
    "            while player_goal_distance < self.size/2:\n",
    "                goal_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "                player_goal_distance = math.sqrt((player_pos[0] - goal_pos[0])**2 + (player_pos[1] - goal_pos[1])**2)\n",
    "        \n",
    "        board[player_pos] = 2\n",
    "        for coord in goals:\n",
    "            if board[coord] == 0:\n",
    "                board[coord] = 3\n",
    "        return board, player_pos, goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-black",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    num_moves = 0\n",
    "    num_backtrack = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        #if reward == -0.25:\n",
    "        #    num_backtrack += 1\n",
    "        if reward == 20:\n",
    "            print(\"found one\")\n",
    "        score += reward\n",
    "        num_moves += 1\n",
    "        \n",
    "    print(f'Episode:{episode} Score:{score} Moves:{num_moves} Backtracks:{num_backtrack}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    env.render()\n",
    "\n",
    "print(f'Episode:{episode} Score:{score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-rough",
   "metadata": {},
   "source": [
    "# 2. Create Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states[0],states[1])))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-rabbit",
   "metadata": {},
   "source": [
    "# 3. Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "#from tf_agents.environments import tf_py_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-california",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#env =  tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_tester(dqn, num_eps, vis = False):\n",
    "    scores = dqn.test(env, nb_episodes=num_eps, visualize=vis)\n",
    "    print(\"Mean Reward: \" + str(np.mean(scores.history['episode_reward'])))\n",
    "    #num_win = 0\n",
    "    #for steps in scores.history['episode_steps']:\n",
    "    #    if steps < 100:\n",
    "    #        num_win += 1\n",
    "    #print(f'Win Rate: {round(num_win/num_eps, 2) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tester(dqn, 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-rwanda",
   "metadata": {},
   "source": [
    "# 4. Saving and Reloading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('saved_models/10x10_maze_easy_redux.h5f', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('saved_models/10x10_maze_easy_redux.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-artwork",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dqn_tester(dqn, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tester(dqn, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-prompt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}