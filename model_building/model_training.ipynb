{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "connected-generator",
   "metadata": {},
   "source": [
    "# 1. Create a custom gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exact-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from enum import Enum\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "balanced-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, size = 10, mode='static'):\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.GAME_LENGTH = size**2\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=0, high=3, shape=(self.size, self.size), dtype=np.int32)\n",
    "        self.state, self.player, self.goals = self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        #self.visited = np.zeros((self.size,self.size), dtype=np.int32)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.time_remaining -= 1\n",
    "        done = False\n",
    "        \n",
    "        #evaluate move, save value of the new space before move then update the state\n",
    "        invalid_move = False\n",
    "        improved = False\n",
    "        if Action(action) == Action.UP:\n",
    "            new_pos = (self.player[0] -1, self.player[1])\n",
    "            if new_pos[0] >= 0 and self.state[new_pos] != 1:\n",
    "                improved = self.averageDistanceToGoalImproved(new_pos)\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.DOWN:\n",
    "            new_pos = (self.player[0] +1, self.player[1])\n",
    "            if new_pos[0] < self.size and self.state[new_pos] != 1:\n",
    "                improved = self.averageDistanceToGoalImproved(new_pos)\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.LEFT:\n",
    "            new_pos = (self.player[0], self.player[1] -1)\n",
    "            if new_pos[1] >= 0 and self.state[new_pos] != 1:\n",
    "                improved = self.averageDistanceToGoalImproved(new_pos)\n",
    "                new_space_val= self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.RIGHT:\n",
    "            new_pos = (self.player[0], self.player[1] +1)\n",
    "            if new_pos[1] < self.size and self.state[new_pos] != 1:\n",
    "                improved = self.averageDistanceToGoalImproved(new_pos)\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        else:\n",
    "            print(\"Invalid input to step function\")\n",
    "            \n",
    "        #evaluate reward \n",
    "        reward = 0\n",
    "        if invalid_move:\n",
    "            reward = -0.5\n",
    "            done = False\n",
    "        else:\n",
    "            if new_space_val == 0:\n",
    "                if(improved):\n",
    "                    reward = 0.04\n",
    "                else:\n",
    "                    reward = -0.04\n",
    "                done = False\n",
    "            elif new_space_val == 3:\n",
    "                self.goals_remaining -= 1\n",
    "                if self.goals == 0:\n",
    "                    reward = 20\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 20\n",
    "                    done = False\n",
    "        \n",
    "        #evaluate if out of time\n",
    "        if self.time_remaining == 0:\n",
    "            done = True\n",
    "            reward = -20\n",
    "            \n",
    "        #placeholder for required return value\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def distanceToGoalImproved(self, new_pos):\n",
    "        distances_before = []\n",
    "        distances_after = []\n",
    "        for goal in self.goals:\n",
    "            distances_before.append(math.sqrt((self.player[0] - goal[0])**2 + (self.player[1] - goal[1])**2))\n",
    "            distances_after.append(math.sqrt((new_pos[0] - goal[0])**2 + (new_pos[1] - goal[1])**2))\n",
    "        index_of_closest = distances_before.index(min(distances_before))\n",
    "        distance_before = distances_before[index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fancy-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, size = 10, mode='static'):\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.GAME_LENGTH = 2 * size**2\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=0, high=3, shape=(self.size, self.size), dtype=np.int32)\n",
    "        self.state, self.player, self.goals = self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        #self.visited = np.zeros((self.size,self.size), dtype=np.int32)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.time_remaining -= 1\n",
    "        done = False\n",
    "        \n",
    "        #evaluate move, save value of the new space before move then update the state\n",
    "        invalid_move = False\n",
    "        improved = False\n",
    "        if Action(action) == Action.UP:\n",
    "            new_pos = (self.player[0] -1, self.player[1])\n",
    "            if new_pos[0] >= 0 and self.state[new_pos] != 1:\n",
    "                #improved = self.distanceToGoalImproved(new_pos)\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.DOWN:\n",
    "            new_pos = (self.player[0] +1, self.player[1])\n",
    "            if new_pos[0] < self.size and self.state[new_pos] != 1:\n",
    "                #improved = self.distanceToGoalImproved(new_pos)\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.LEFT:\n",
    "            new_pos = (self.player[0], self.player[1] -1)\n",
    "            if new_pos[1] >= 0 and self.state[new_pos] != 1:\n",
    "                #improved = self.distanceToGoalImproved(new_pos)\n",
    "                new_space_val= self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        elif Action(action) == Action.RIGHT:\n",
    "            new_pos = (self.player[0], self.player[1] +1)\n",
    "            if new_pos[1] < self.size and self.state[new_pos] != 1:\n",
    "                #improved = self.distanceToGoalImproved(new_pos)\n",
    "                new_space_val = self.evalMove(new_pos)\n",
    "            else:\n",
    "                invalid_move = True\n",
    "        else:\n",
    "            print(\"Invalid input to step function\")\n",
    "            \n",
    "        #evaluate reward \n",
    "        reward = 0\n",
    "        if invalid_move:\n",
    "            reward = -0.5\n",
    "            done = False\n",
    "        else:\n",
    "            if new_space_val == 0:\n",
    "                #if(improved):\n",
    "                #    reward = 0.04\n",
    "                #else:\n",
    "                reward = -0.04\n",
    "                done = False\n",
    "            elif new_space_val == 3:\n",
    "                self.goals_remaining -= 1\n",
    "                if self.goals_remaining == 0:\n",
    "                    reward = 20\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 20\n",
    "                    done = False\n",
    "        \n",
    "        #evaluate if out of time\n",
    "        if self.time_remaining == 0:\n",
    "            done = True\n",
    "            reward = -20\n",
    "            \n",
    "        #placeholder for required return value\n",
    "        info = {}\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def distanceToGoalImproved(self, new_pos):\n",
    "        distances_before = []\n",
    "        distances_after = []\n",
    "        for goal in self.goals:\n",
    "            distances_before.append(math.sqrt((self.player[0] - goal[0])**2 + (self.player[1] - goal[1])**2))\n",
    "            distances_after.append(math.sqrt((new_pos[0] - goal[0])**2 + (new_pos[1] - goal[1])**2))\n",
    "        index_of_closest = distances_before.index(min(distances_before))\n",
    "        distance_change = distances_after[index_of_closest] - distances_before[index_of_closest]\n",
    "        if distance_change < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def evalMove(self, new_pos):\n",
    "        # check if we back_tracked\n",
    "        #if self.visited[new_pos] == -1:\n",
    "        #    back_track = True\n",
    "        #else:\n",
    "        #    back_track = False\n",
    "        #update the matrix of visited cells\n",
    "        #self.visited[self.player] = -1\n",
    "        #process the move\n",
    "        new_space_val = self.state[new_pos]\n",
    "        self.state[self.player] = 0\n",
    "        self.state[new_pos] = 2\n",
    "        self.player = new_pos\n",
    "        return new_space_val\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.title(f'Clock: {self.time_remaining}, Goals: {self.goals_remaining}')\n",
    "        plt.imshow(self.state)\n",
    "        plt.show()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state, self.player, self.goals =  self.createBoard()\n",
    "        self.goals_remaining = len(self.goals)\n",
    "        self.time_remaining = self.GAME_LENGTH\n",
    "        return self.state\n",
    "    \n",
    "    def createBoard(self):\n",
    "        board = np.matrix([[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,0,0,0,0,0,0,0,0,1],\n",
    "                 [1,1,1,0,1,1,1,0,1,1],\n",
    "                 [1,0,0,0,0,1,0,0,0,1],\n",
    "                 [1,0,1,1,1,1,0,1,0,1],\n",
    "                 [1,0,1,1,0,3,0,1,0,1],\n",
    "                 [1,0,0,0,0,0,0,1,0,1],\n",
    "                 [1,1,1,0,1,0,1,1,0,1],\n",
    "                 [1,3,0,0,1,0,0,0,3,1],\n",
    "                 [1,1,1,1,1,1,1,1,1,1]])\n",
    "        if self.mode == 'static':\n",
    "            player_pos = (1,1) #(np.random.randint(self.size), np.random.randint(self.size))\n",
    "            goals = [(5,5),(8,1),(8,8)]\n",
    "        elif self.mode == 'random':\n",
    "            player_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            goal_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "            player_goal_distance = math.sqrt((player_pos[0] - goal_pos[0])**2 + (player_pos[1] - goal_pos[1])**2)\n",
    "            while player_goal_distance < self.size/2:\n",
    "                goal_pos = (np.random.randint(self.size), np.random.randint(self.size))\n",
    "                player_goal_distance = math.sqrt((player_pos[0] - goal_pos[0])**2 + (player_pos[1] - goal_pos[1])**2)\n",
    "        \n",
    "        board[player_pos] = 2\n",
    "        for coord in goals:\n",
    "            if board[coord] == 0:\n",
    "                board[coord] = 3\n",
    "        return board, player_pos, goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-black",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "moving-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found one\n",
      "Episode:1 Score:-49.77999999999992 Moves:200 Backtracks:0\n",
      "Episode:2 Score:-73.03999999999992 Moves:200 Backtracks:0\n",
      "Episode:3 Score:-69.35999999999993 Moves:200 Backtracks:0\n",
      "Episode:4 Score:-76.71999999999994 Moves:200 Backtracks:0\n",
      "found one\n",
      "Episode:5 Score:-49.77999999999991 Moves:200 Backtracks:0\n",
      "Episode:6 Score:-75.79999999999994 Moves:200 Backtracks:0\n",
      "Episode:7 Score:-75.33999999999992 Moves:200 Backtracks:0\n",
      "Episode:8 Score:-67.51999999999992 Moves:200 Backtracks:0\n",
      "found one\n",
      "Episode:9 Score:-52.539999999999914 Moves:200 Backtracks:0\n",
      "found one\n",
      "Episode:10 Score:-49.77999999999992 Moves:200 Backtracks:0\n",
      "found one\n",
      "Episode:11 Score:-53.91999999999993 Moves:200 Backtracks:0\n",
      "found one\n",
      "Episode:12 Score:-52.07999999999992 Moves:200 Backtracks:0\n",
      "Episode:13 Score:-75.79999999999993 Moves:200 Backtracks:0\n",
      "found one\n",
      "Episode:14 Score:-54.83999999999992 Moves:200 Backtracks:0\n",
      "Episode:15 Score:-78.55999999999993 Moves:200 Backtracks:0\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "episodes = 15\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    num_moves = 0\n",
    "    num_backtrack = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        #if reward == -0.25:\n",
    "        #    num_backtrack += 1\n",
    "        if reward == 20:\n",
    "            print(\"found one\")\n",
    "        score += reward\n",
    "        num_moves += 1\n",
    "        \n",
    "    print(f'Episode:{episode} Score:{score} Moves:{num_moves} Backtracks:{num_backtrack}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "trained-folder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:15 Score:-69.35999999999993\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "score = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    env.render()\n",
    "\n",
    "print(f'Episode:{episode} Score:{score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-rough",
   "metadata": {},
   "source": [
    "# 2. Create Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "southern-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "exterior-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states[0],states[1])))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "packed-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "thirty-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "model = build_model(states,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "better-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 20,604\n",
      "Trainable params: 20,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-rabbit",
   "metadata": {},
   "source": [
    "# 3. Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "elder-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "#from tf_agents.environments import tf_py_environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "linear-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=100000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                   nb_actions=actions, nb_steps_warmup=100, target_model_update=1 e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "infectious-california",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -0.1380 0s - reward: -\n",
      "50 episodes - episode_reward: -27.609 [-72.120, -8.320] - loss: 2.452 - mae: 11.406 - mean_q: 15.708\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.1178\n",
      "50 episodes - episode_reward: -23.550 [-40.580, -10.880] - loss: 1.790 - mae: 12.436 - mean_q: 14.570\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 56s 6ms/step - reward: -0.1141\n",
      "50 episodes - episode_reward: -22.828 [-51.620, -0.960] - loss: 1.142 - mae: 11.903 - mean_q: 11.455\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -0.0777\n",
      "50 episodes - episode_reward: -15.549 [-31.120, 4.100] - loss: 1.177 - mae: 12.785 - mean_q: 11.840\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -0.0935\n",
      "50 episodes - episode_reward: -18.695 [-41.960, 1.340] - loss: 1.603 - mae: 14.994 - mean_q: 15.979\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -0.0453\n",
      "50 episodes - episode_reward: -9.052 [-53.460, 7.320] - loss: 1.552 - mae: 14.171 - mean_q: 15.448\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -0.0778\n",
      "50 episodes - episode_reward: -15.570 [-48.860, 6.860] - loss: 2.052 - mae: 17.833 - mean_q: 21.147\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -0.0733\n",
      "50 episodes - episode_reward: -14.662 [-46.560, 6.860] - loss: 1.829 - mae: 16.937 - mean_q: 18.966\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -0.0398\n",
      "50 episodes - episode_reward: -7.965 [-47.020, 8.240] - loss: 1.537 - mae: 15.809 - mean_q: 17.011\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -0.0322\n",
      "done, took 625.837 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16cc3e60dd8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env =  tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=100000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "expanded-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_tester(dqn, num_eps, vis = False):\n",
    "    scores = dqn.test(env, nb_episodes=num_eps, visualize=vis)\n",
    "    print(\"Mean Reward: \" + str(np.mean(scores.history['episode_reward'])))\n",
    "    #num_win = 0\n",
    "    #for steps in scores.history['episode_steps']:\n",
    "    #    if steps < 100:\n",
    "    #        num_win += 1\n",
    "    #print(f'Win Rate: {round(num_win/num_eps, 2) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "figured-glucose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: 32.160, steps: 200\n",
      "Episode 2: reward: 32.160, steps: 200\n",
      "Episode 3: reward: 32.160, steps: 200\n",
      "Episode 4: reward: 32.160, steps: 200\n",
      "Episode 5: reward: 32.160, steps: 200\n",
      "Episode 6: reward: 32.160, steps: 200\n",
      "Episode 7: reward: 32.160, steps: 200\n",
      "Episode 8: reward: 32.160, steps: 200\n",
      "Episode 9: reward: 32.160, steps: 200\n",
      "Episode 10: reward: 32.160, steps: 200\n",
      "Episode 11: reward: 32.160, steps: 200\n",
      "Episode 12: reward: 32.160, steps: 200\n",
      "Episode 13: reward: 32.160, steps: 200\n",
      "Episode 14: reward: 32.160, steps: 200\n",
      "Episode 15: reward: 32.160, steps: 200\n",
      "Mean Reward: 32.16000000000015\n"
     ]
    }
   ],
   "source": [
    "dqn_tester(dqn, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-rwanda",
   "metadata": {},
   "source": [
    "# 4. Saving and Reloading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "right-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('saved_models/10x10_maze_easy.h5f', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "inner-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "seventh-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "sunset-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('saved_models/10x10_maze_easy.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "rolled-artwork",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: 58.920, steps: 30\n",
      "Episode 2: reward: 58.920, steps: 30\n",
      "Episode 3: reward: 58.920, steps: 30\n",
      "Episode 4: reward: 58.920, steps: 30\n",
      "Episode 5: reward: 58.920, steps: 30\n",
      "Episode 6: reward: 58.920, steps: 30\n",
      "Episode 7: reward: 58.920, steps: 30\n",
      "Episode 8: reward: 58.920, steps: 30\n",
      "Episode 9: reward: 58.920, steps: 30\n",
      "Episode 10: reward: 58.920, steps: 30\n",
      "Episode 11: reward: 58.920, steps: 30\n",
      "Episode 12: reward: 58.920, steps: 30\n",
      "Episode 13: reward: 58.920, steps: 30\n",
      "Episode 14: reward: 58.920, steps: 30\n",
      "Episode 15: reward: 58.920, steps: 30\n",
      "Mean Reward: 58.920000000000044\n"
     ]
    }
   ],
   "source": [
    "dqn_tester(dqn, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "closing-survival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 58.920, steps: 30\n",
      "Mean Reward: 58.920000000000016\n"
     ]
    }
   ],
   "source": [
    "dqn_tester(dqn, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-prompt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:final_project] *",
   "language": "python",
   "name": "conda-env-final_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
